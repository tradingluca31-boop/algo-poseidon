"""
Ensemble Methods pour amÃ©liorer les prÃ©dictions ML
Combine LightGBM + XGBoost + CatBoost via Voting Classifier
Version 1.0 - 2025-10-09

OBJECTIF:
- Combiner 3 modÃ¨les de gradient boosting
- Voting majoritaire pondÃ©rÃ©
- AmÃ©liorer robustesse et gÃ©nÃ©ralisation
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import joblib
import os
from datetime import datetime

print("=" * 80)
print("ðŸŽ¯ ENSEMBLE METHODS - LIGHTGBM + XGBOOST + CATBOOST")
print("=" * 80)

# ==================== CONFIGURATION ====================
INPUT_CSV = "XAUUSD_COMPLETE_ML_Data_20Y_ENGINEERED.csv"
MODEL_OUTPUT = "xauusd_ensemble_model.pkl"
N_SPLITS = 3  # Cross-validation folds

# ==================== CHARGEMENT ====================
print("\nðŸ“‚ Chargement des donnÃ©es...")
csv_path = os.path.join(r"C:\Users\lbye3\AppData\Roaming\MetaQuotes\Terminal\Common\Files", INPUT_CSV)

if not os.path.exists(csv_path):
    print(f"âŒ Fichier introuvable: {csv_path}")
    print(f"Lancez d'abord: python feature_engineering_advanced.py")
    exit(1)

df = pd.read_csv(csv_path)
print(f"âœ… {len(df)} lignes chargÃ©es")

# PrÃ©parer les donnÃ©es
df_valid = df[df['target'].isin([0, 1])].copy()
print(f"âœ… {len(df_valid)} lignes avec target valide")

# Undersampling pour Ã©quilibrer
df_win = df_valid[df_valid['target'] == 1].copy()
df_loss = df_valid[df_valid['target'] == 0].copy()
df_loss_sampled = df_loss.sample(n=len(df_win), random_state=42)
df_balanced = pd.concat([df_win, df_loss_sampled], ignore_index=True)
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

print(f"âœ… Dataset Ã©quilibrÃ©: {len(df_balanced)} lignes")

# SÃ©parer X et y
exclude_cols = ['target', 'time']
feature_cols = [col for col in df_balanced.columns if col not in exclude_cols]

X = df_balanced[feature_cols].copy()
y = df_balanced['target'].copy()

X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

print(f"âœ… {X.shape[0]} lignes Ã— {X.shape[1]} features")

# ==================== CONFIGURATION MODÃˆLES ====================
print("\n" + "=" * 80)
print("âš™ï¸ CONFIGURATION DES 3 MODÃˆLES")
print("=" * 80)

# LightGBM - Rapide et efficace
lgbm_params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 50,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'min_child_samples': 20,
    'max_depth': 7,
    'lambda_l1': 0.1,
    'lambda_l2': 0.1,
    'verbosity': -1,
    'random_state': 42,
    'n_estimators': 500
}

# XGBoost - Puissant et prÃ©cis
xgb_params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'max_depth': 7,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 20,
    'reg_alpha': 0.1,
    'reg_lambda': 0.1,
    'random_state': 42,
    'n_estimators': 500,
    'tree_method': 'hist'
}

# CatBoost - GÃ¨re bien les features catÃ©gorielles
cat_params = {
    'iterations': 500,
    'learning_rate': 0.05,
    'depth': 7,
    'l2_leaf_reg': 0.1,
    'subsample': 0.8,
    'random_seed': 42,
    'verbose': False,
    'loss_function': 'Logloss',
    'eval_metric': 'AUC'
}

print("âœ… LightGBM configurÃ©")
print("âœ… XGBoost configurÃ©")
print("âœ… CatBoost configurÃ©")

# ==================== ENTRAÃŽNEMENT INDIVIDUEL ====================
print("\n" + "=" * 80)
print("ðŸš€ ENTRAÃŽNEMENT DES 3 MODÃˆLES INDIVIDUELS")
print("=" * 80)

# Split train/test temporel
split_idx = int(len(X) * 0.8)
X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\nðŸ“Š Split temporel:")
print(f"   Train: {len(X_train)} lignes ({len(X_train)/len(X)*100:.1f}%)")
print(f"   Test:  {len(X_test)} lignes ({len(X_test)/len(X)*100:.1f}%)")

# 1. LightGBM
print("\nðŸ”µ EntraÃ®nement LightGBM...")
lgbm_model = lgb.LGBMClassifier(**lgbm_params)
lgbm_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
)
lgbm_pred = lgbm_model.predict_proba(X_test)[:, 1]
lgbm_auc = roc_auc_score(y_test, lgbm_pred)
print(f"   âœ… LightGBM AUC: {lgbm_auc:.4f}")

# 2. XGBoost
print("\nðŸŸ  EntraÃ®nement XGBoost...")
xgb_model = xgb.XGBClassifier(**xgb_params)
xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=50,
    verbose=False
)
xgb_pred = xgb_model.predict_proba(X_test)[:, 1]
xgb_auc = roc_auc_score(y_test, xgb_pred)
print(f"   âœ… XGBoost AUC: {xgb_auc:.4f}")

# 3. CatBoost
print("\nðŸŸ¢ EntraÃ®nement CatBoost...")
cat_model = cb.CatBoostClassifier(**cat_params)
cat_model.fit(
    X_train, y_train,
    eval_set=(X_test, y_test),
    early_stopping_rounds=50,
    verbose=False
)
cat_pred = cat_model.predict_proba(X_test)[:, 1]
cat_auc = roc_auc_score(y_test, cat_pred)
print(f"   âœ… CatBoost AUC: {cat_auc:.4f}")

# ==================== ENSEMBLE VOTING ====================
print("\n" + "=" * 80)
print("ðŸŽ¯ CRÃ‰ATION ENSEMBLE VOTING CLASSIFIER")
print("=" * 80)

# PondÃ©ration basÃ©e sur les performances individuelles
total_auc = lgbm_auc + xgb_auc + cat_auc
lgbm_weight = lgbm_auc / total_auc
xgb_weight = xgb_auc / total_auc
cat_weight = cat_auc / total_auc

print(f"\nðŸ“Š Poids calculÃ©s:")
print(f"   LightGBM: {lgbm_weight:.3f}")
print(f"   XGBoost:  {xgb_weight:.3f}")
print(f"   CatBoost: {cat_weight:.3f}")

# CrÃ©er l'ensemble
ensemble = VotingClassifier(
    estimators=[
        ('lgbm', lgbm_model),
        ('xgb', xgb_model),
        ('cat', cat_model)
    ],
    voting='soft',  # Moyenne des probabilitÃ©s
    weights=[lgbm_weight, xgb_weight, cat_weight]
)

# Fit sur les modÃ¨les dÃ©jÃ  entraÃ®nÃ©s (voting se base sur predict_proba)
print("\nðŸ”„ CrÃ©ation de l'ensemble...")
ensemble.estimators_ = [lgbm_model, xgb_model, cat_model]
ensemble.classes_ = np.array([0, 1])

# PrÃ©dictions ensemble
ensemble_pred = ensemble.predict_proba(X_test)[:, 1]
ensemble_auc = roc_auc_score(y_test, ensemble_pred)

print(f"\nðŸ† RÃ‰SULTATS ENSEMBLE:")
print(f"   AUC Ensemble: {ensemble_auc:.4f}")

# Comparaison
print(f"\nðŸ“Š Comparaison avec modÃ¨les individuels:")
print(f"   LightGBM: {lgbm_auc:.4f}")
print(f"   XGBoost:  {xgb_auc:.4f}")
print(f"   CatBoost: {cat_auc:.4f}")
print(f"   Ensemble: {ensemble_auc:.4f} {'âœ… MEILLEUR' if ensemble_auc > max(lgbm_auc, xgb_auc, cat_auc) else ''}")

# ==================== MÃ‰TRIQUES DÃ‰TAILLÃ‰ES ====================
print("\n" + "=" * 80)
print("ðŸ“Š MÃ‰TRIQUES DÃ‰TAILLÃ‰ES ENSEMBLE")
print("=" * 80)

ensemble_pred_binary = (ensemble_pred >= 0.5).astype(int)

print("\nðŸ“‹ Classification Report:")
print(classification_report(y_test, ensemble_pred_binary, target_names=['LOSS', 'WIN']))

print("\nðŸ“Š Confusion Matrix:")
cm = confusion_matrix(y_test, ensemble_pred_binary)
print(f"   True Negatives:  {cm[0, 0]}")
print(f"   False Positives: {cm[0, 1]}")
print(f"   False Negatives: {cm[1, 0]}")
print(f"   True Positives:  {cm[1, 1]}")

win_rate = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0
print(f"\nðŸŽ¯ Win Rate (sur signaux prÃ©dits WIN): {win_rate*100:.2f}%")

# ==================== SAUVEGARDE ====================
print("\n" + "=" * 80)
print("ðŸ’¾ SAUVEGARDE MODÃˆLE ENSEMBLE")
print("=" * 80)

model_path = os.path.join(os.path.dirname(csv_path), MODEL_OUTPUT)

model_data = {
    'ensemble': ensemble,
    'lgbm_model': lgbm_model,
    'xgb_model': xgb_model,
    'cat_model': cat_model,
    'feature_cols': feature_cols,
    'weights': [lgbm_weight, xgb_weight, cat_weight],
    'individual_aucs': {
        'lgbm': lgbm_auc,
        'xgb': xgb_auc,
        'cat': cat_auc
    },
    'ensemble_auc': ensemble_auc,
    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'n_samples': len(X),
    'n_features': len(feature_cols),
    'ensemble_method': 'soft_voting'
}

joblib.dump(model_data, model_path)
print(f"âœ… ModÃ¨le sauvegardÃ©: {model_path}")

file_size_mb = os.path.getsize(model_path) / (1024 * 1024)
print(f"ðŸ“¦ Taille: {file_size_mb:.2f} MB")

# ==================== RÃ‰SUMÃ‰ ====================
print("\n" + "=" * 80)
print("âœ… ENSEMBLE METHODS TERMINÃ‰")
print("=" * 80)

print(f"\nðŸ“Š RÃ©sumÃ©:")
print(f"   ModÃ¨les combinÃ©s: 3 (LightGBM, XGBoost, CatBoost)")
print(f"   MÃ©thode: Soft Voting (moyenne pondÃ©rÃ©e des probabilitÃ©s)")
print(f"   AUC Final: {ensemble_auc:.4f}")
print(f"   Features: {len(feature_cols)}")
print(f"   Samples: {len(X)}")

improvement = ensemble_auc - max(lgbm_auc, xgb_auc, cat_auc)
print(f"\nðŸŽ¯ AmÃ©lioration vs meilleur modÃ¨le seul:")
print(f"   {improvement:+.4f} AUC")
print(f"   {improvement*100:+.2f}% d'amÃ©lioration" if improvement > 0 else f"   Pas d'amÃ©lioration")

print(f"\nðŸš€ Prochaines Ã©tapes:")
print(f"   1. Tester avec: python calibration_70_percent.py")
print(f"   2. Calibrer pour obtenir 70%+ confidence threshold")
print(f"   3. Backtest final")

print("\n" + "=" * 80)
